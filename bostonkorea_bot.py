#!/usr/bin/env python3
"""
ë³´ìŠ¤í†¤ì½”ë¦¬ì•„ ê¸°ì‚¬ â†’ ì†Œì…œë¯¸ë””ì–´ í¬ìŠ¤íŒ… ë´‡
ì¸ìŠ¤íƒ€ê·¸ë¨ê³¼ X(íŠ¸ìœ„í„°)ìš© í…ìŠ¤íŠ¸ë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import unquote
import re
import sys
import pyperclip

class BostonKoreaBot:
    BASE_URL = 'https://www.bostonkorea.com'

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def fetch_latest_articles(self, category: str = None, limit: int = 15) -> list:
        """ìµœì‹  ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        if category:
            url = f"{self.BASE_URL}/bbs/board.php?bo_table=news&sca={category}"
        else:
            url = f"{self.BASE_URL}/bbs/board.php?bo_table=news"

        response = requests.get(url, headers=self.headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')

        articles = []

        # webzineList êµ¬ì¡°ì—ì„œ ê¸°ì‚¬ ì¶”ì¶œ (ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ë§Œ)
        # ì‚¬ì´ë“œë°”(.hot_issue ë“±) ì œì™¸
        main_content = soup.select_one('#bo_list, .board_list, #container')
        if main_content:
            items = main_content.select('.webzineList > ul > li')
        else:
            items = soup.select('.webzineList > ul > li')

        for item in items[:limit]:
            link = item.select_one('a')
            if not link:
                continue

            href = link.get('href', '')
            if not href or '/news/' not in href:
                continue

            # ì œëª© ì¶”ì¶œ (strong íƒœê·¸ì—ì„œ)
            title_elem = item.select_one('.bo_tit strong')
            if title_elem:
                title = title_elem.get_text(strip=True)
            else:
                # ëŒ€ì²´: bo_titì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸
                tit_elem = item.select_one('.bo_tit')
                title = tit_elem.get_text(strip=True) if tit_elem else ''

            # ì œëª© ì• ìˆ«ì ì œê±° (ì˜ˆ: "1.ì œëª©" â†’ "ì œëª©")
            title = re.sub(r'^\d+\.', '', title).strip()

            if not title or len(title) < 5:
                continue

            full_url = href if href.startswith('http') else self.BASE_URL + href

            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            cat_elem = item.select_one('.bo_tit em')
            category_text = ''
            if cat_elem:
                # em íƒœê·¸ì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ì¶œ
                em_text = cat_elem.get_text(strip=True)
                # ë³¸ë¬¸ ìš”ì•½ì´ ì•„ë‹Œ ì§§ì€ ì¹´í…Œê³ ë¦¬ë§Œ
                if len(em_text) < 20:
                    category_text = em_text

            articles.append({
                'title': title[:55] + '...' if len(title) > 55 else title,
                'url': full_url,
                'category': category_text
            })

        # webzineListê°€ ì—†ìœ¼ë©´ ëŒ€ì²´ ë°©ì‹
        if not articles and not category:
            links = soup.select('a[href*="/news/"]')
            seen_urls = set()
            for link in links:
                href = link.get('href', '')
                title_elem = link.select_one('strong') or link
                title = title_elem.get_text(strip=True)

                if '/news/' in href and href not in seen_urls and title and len(title) > 10:
                    full_url = href if href.startswith('http') else self.BASE_URL + href
                    seen_urls.add(href)
                    articles.append({
                        'title': title[:55] + '...' if len(title) > 55 else title,
                        'url': full_url,
                        'category': ''
                    })
                    if len(articles) >= limit:
                        break

        return articles

    def get_categories(self) -> dict:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ëª©ë¡"""
        return {
            'ì „ì²´': '',
            'ë¯¸êµ­': 'ë¯¸êµ­',
            'í•œêµ­': 'í•œêµ­',
            'ê²½ì œ': 'ê²½ì œ',
            'ë¹„ì¦ˆë‹ˆìŠ¤': 'ë¹„ì¦ˆë‹ˆìŠ¤',
            'ë¯¸êµ­ì£¼ì‹': 'ë¯¸êµ­ì£¼ì‹',
            'ìŠ¤í¬ì¸ ': 'ìŠ¤í¬ì¸ ',
            'êµìœ¡ìœ í•™': 'êµìœ¡ìœ í•™',
            'ì¹¼ëŸ¼': 'ì¹¼ëŸ¼',
        }

    def fetch_article(self, url: str) -> dict:
        """ê¸°ì‚¬ URLì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        response = requests.get(url, headers=self.headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')

        # ì œëª© ì¶”ì¶œ
        title_elem = soup.select_one('#bo_v_title .bo_v_tit, h2.bo_v_tit')
        title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"

        # ë³¸ë¬¸ ì¶”ì¶œ
        content_elem = soup.select_one('#bo_v_con')
        if content_elem:
            # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°
            for tag in content_elem.find_all(['script', 'style', 'iframe']):
                tag.decompose()
            content = content_elem.get_text(separator='\n', strip=True)
            # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ
            content = re.sub(r'\n{3,}', '\n\n', content)
        else:
            content = ""

        # ì´ë¯¸ì§€ ì¶”ì¶œ
        img_elem = soup.select_one('#bo_v_img img, #bo_v_con img')
        image_url = img_elem.get('src') if img_elem else None

        # ë‚ ì§œ ì¶”ì¶œ
        date_elem = soup.select_one('.bo_v_info')
        date_text = ""
        if date_elem:
            date_match = re.search(r'(\d{2}-\d{2}-\d{2}\s+\d{2}:\d{2})', date_elem.get_text())
            if date_match:
                date_text = date_match.group(1)

        # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        category = ""
        breadcrumb = soup.select('.bo_v_cate a, .bo_tit em')
        if breadcrumb:
            category = breadcrumb[-1].get_text(strip=True)

        return {
            'url': url,
            'title': title,
            'content': content[:2000],  # ì²˜ìŒ 2000ìë§Œ
            'image_url': image_url,
            'date': date_text,
            'category': category
        }

    def summarize_content(self, content: str, max_length: int = 200) -> str:
        """ë³¸ë¬¸ ìš”ì•½ (ì²« ë¬¸ì¥ë“¤ ì¶”ì¶œ)"""
        sentences = re.split(r'[.ã€‚]\s*', content)
        summary = ""
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            if len(summary) + len(sentence) + 2 > max_length:
                break
            summary += sentence + ". "
        return summary.strip()

    def generate_hashtags(self, title: str, category: str) -> str:
        """í•´ì‹œíƒœê·¸ ìƒì„±"""
        tags = ['#ë³´ìŠ¤í†¤ì½”ë¦¬ì•„', '#ë³´ìŠ¤í†¤', '#Boston', '#ë¯¸êµ­ë‰´ìŠ¤']

        # ì¹´í…Œê³ ë¦¬ë³„ íƒœê·¸ ì¶”ê°€
        category_tags = {
            'ê²½ì œ': ['#ê²½ì œ', '#ë¹„ì¦ˆë‹ˆìŠ¤'],
            'ë¹„ì¦ˆë‹ˆìŠ¤': ['#ë¹„ì¦ˆë‹ˆìŠ¤', '#ê²½ì œë‰´ìŠ¤'],
            'ë¯¸êµ­ì£¼ì‹': ['#ë¯¸êµ­ì£¼ì‹', '#ì£¼ì‹', '#íˆ¬ì'],
            'ìŠ¤í¬ì¸ ': ['#ìŠ¤í¬ì¸ ', '#MLB', '#NBA'],
            'í•œêµ­': ['#í•œêµ­ë‰´ìŠ¤', '#Korea'],
            'êµìœ¡': ['#êµìœ¡', '#ìœ í•™'],
            'ë¶€ë™ì‚°': ['#ë¶€ë™ì‚°', '#ë¯¸êµ­ë¶€ë™ì‚°'],
        }

        for key, extra_tags in category_tags.items():
            if key in category:
                tags.extend(extra_tags)
                break

        return ' '.join(tags[:8])  # ìµœëŒ€ 8ê°œ

    def format_for_x(self, article: dict) -> str:
        """X(íŠ¸ìœ„í„°)ìš© í¬ë§· ìƒì„± (280ì ì œí•œ)"""
        title = article['title']
        url = article['url']
        hashtags = self.generate_hashtags(title, article['category'])

        # URLì€ ì•½ 23ìë¡œ ê³„ì‚° (t.co ë‹¨ì¶•)
        url_length = 23
        hashtag_length = len(hashtags)
        available = 280 - url_length - hashtag_length - 4  # ì—¬ìœ  ê³µê°„

        if len(title) > available:
            title = title[:available-3] + "..."

        return f"{title}\n\n{url}\n\n{hashtags}"

    def format_for_instagram(self, article: dict) -> str:
        """ì¸ìŠ¤íƒ€ê·¸ë¨ìš© í¬ë§· ìƒì„±"""
        title = article['title']
        summary = self.summarize_content(article['content'], 300)
        hashtags = self.generate_hashtags(title, article['category'])

        # ì¸ìŠ¤íƒ€ê·¸ë¨ìš© ì¶”ê°€ í•´ì‹œíƒœê·¸
        extra_tags = '#ë‰´ìŠ¤ #ë¯¸ì£¼í•œì¸ #ì¬ë¯¸êµí¬ #í•œì¸ì»¤ë®¤ë‹ˆí‹° #ë³´ìŠ¤í†¤í•œì¸'

        post = f"""ğŸ“° {title}

{summary}

ğŸ‘‰ ìì„¸í•œ ë‚´ìš©ì€ í”„ë¡œí•„ ë§í¬ì—ì„œ í™•ì¸í•˜ì„¸ìš”!

{hashtags}
{extra_tags}"""

        return post

    def format_for_both(self, article: dict) -> dict:
        """ë‘ í”Œë«í¼ìš© í¬ë§· ëª¨ë‘ ìƒì„±"""
        return {
            'x': self.format_for_x(article),
            'instagram': self.format_for_instagram(article),
            'image_url': article.get('image_url', '')
        }


def process_article(bot, url):
    """ê¸°ì‚¬ ì²˜ë¦¬ ë° ì¶œë ¥"""
    print("\nâ³ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    article = bot.fetch_article(url)
    result = bot.format_for_both(article)

    print("\n" + "=" * 50)
    print("ğŸ“Œ ê¸°ì‚¬ ì •ë³´")
    print("=" * 50)
    print(f"ì œëª©: {article['title']}")
    print(f"ì¹´í…Œê³ ë¦¬: {article['category']}")
    print(f"ë‚ ì§œ: {article['date']}")

    print("\n" + "-" * 50)
    print("ğŸ¦ X (íŠ¸ìœ„í„°)ìš© í…ìŠ¤íŠ¸:")
    print("-" * 50)
    print(result['x'])
    print(f"\nğŸ“ ê¸€ì ìˆ˜: {len(result['x'])}ì")

    print("\n" + "-" * 50)
    print("ğŸ“¸ ì¸ìŠ¤íƒ€ê·¸ë¨ìš© í…ìŠ¤íŠ¸:")
    print("-" * 50)
    print(result['instagram'])

    if result['image_url']:
        print(f"\nğŸ–¼ï¸  ì´ë¯¸ì§€ URL: {result['image_url']}")

    # í´ë¦½ë³´ë“œ ë³µì‚¬ ì˜µì…˜
    print("\n" + "-" * 50)
    print("ë³µì‚¬í•  í…ìŠ¤íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. Xìš© í…ìŠ¤íŠ¸")
    print("2. ì¸ìŠ¤íƒ€ê·¸ë¨ìš© í…ìŠ¤íŠ¸")
    print("3. ê±´ë„ˆë›°ê¸°")

    copy_choice = input("\nì„ íƒ (1-3): ").strip()

    if copy_choice == '1':
        try:
            pyperclip.copy(result['x'])
            print("âœ… Xìš© í…ìŠ¤íŠ¸ê°€ í´ë¦½ë³´ë“œì— ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        except:
            print("âš ï¸  í´ë¦½ë³´ë“œ ë³µì‚¬ ì‹¤íŒ¨. í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë³µì‚¬í•˜ì„¸ìš”.")
    elif copy_choice == '2':
        try:
            pyperclip.copy(result['instagram'])
            print("âœ… ì¸ìŠ¤íƒ€ê·¸ë¨ìš© í…ìŠ¤íŠ¸ê°€ í´ë¦½ë³´ë“œì— ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        except:
            print("âš ï¸  í´ë¦½ë³´ë“œ ë³µì‚¬ ì‹¤íŒ¨. í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë³µì‚¬í•˜ì„¸ìš”.")


def show_article_list(bot):
    """ìµœì‹  ê¸°ì‚¬ ëª©ë¡ í‘œì‹œ ë° ì„ íƒ"""
    categories = bot.get_categories()

    print("\n" + "=" * 50)
    print("ğŸ“‚ ì¹´í…Œê³ ë¦¬ ì„ íƒ")
    print("=" * 50)

    cat_list = list(categories.keys())
    for i, cat in enumerate(cat_list, 1):
        print(f"{i}. {cat}")

    cat_choice = input("\nì¹´í…Œê³ ë¦¬ ì„ íƒ (ê¸°ë³¸: ì „ì²´): ").strip()

    try:
        cat_idx = int(cat_choice) - 1 if cat_choice else 0
        selected_cat = categories[cat_list[cat_idx]]
    except:
        selected_cat = ''

    print("\nâ³ ìµœì‹  ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    articles = bot.fetch_latest_articles(category=selected_cat, limit=15)

    if not articles:
        print("âŒ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    print("\n" + "=" * 50)
    print(f"ğŸ“° ìµœì‹  ê¸°ì‚¬ ëª©ë¡ ({len(articles)}ê°œ)")
    print("=" * 50)

    for i, article in enumerate(articles, 1):
        cat_tag = f"[{article['category']}] " if article['category'] else ""
        print(f"{i:2}. {cat_tag}{article['title']}")

    print("\n0. ë©”ì¸ ë©”ë‰´ë¡œ ëŒì•„ê°€ê¸°")

    article_choice = input("\nê¸°ì‚¬ ë²ˆí˜¸ ì„ íƒ: ").strip()

    try:
        idx = int(article_choice)
        if idx == 0:
            return None
        if 1 <= idx <= len(articles):
            return articles[idx - 1]['url']
    except:
        pass

    print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    return None


def main():
    bot = BostonKoreaBot()

    print("=" * 50)
    print("ğŸ—ï¸  ë³´ìŠ¤í†¤ì½”ë¦¬ì•„ ì†Œì…œë¯¸ë””ì–´ í¬ìŠ¤íŒ… ë´‡")
    print("=" * 50)

    while True:
        print("\nì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ìµœì‹  ê¸°ì‚¬ ëª©ë¡ ë³´ê¸°")
        print("2. ê¸°ì‚¬ URL ì§ì ‘ ì…ë ¥")
        print("3. ì¢…ë£Œ")

        choice = input("\nì„ íƒ (1-3): ").strip()

        if choice == '3':
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if choice == '1':
            url = show_article_list(bot)
            if url:
                try:
                    process_article(bot, url)
                except Exception as e:
                    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

        if choice != '2':
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            continue

        url = input("\nê¸°ì‚¬ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()

        if not url.startswith('http'):
            print("âŒ ì˜¬ë°”ë¥¸ URLì„ ì…ë ¥í•˜ì„¸ìš”.")
            continue

        try:
            process_article(bot, url)
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue


if __name__ == "__main__":
    main()
